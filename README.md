# seismic multiplets scan routine

The repository contains:

* **mathematica_code.nb**; the originally written *mathematica code notebook*. This is the software complete with all features and an interactive GUI. The notebook runs in a licensed Wolfram Mathematica environment (https://www.wolfram.com/mathematica) or in the Wolfram Player Pro (https://www.wolfram.com/player-pro/), the commercial paid version with I/O enabled.

* **Wolfram_CDF_example.cdf.zip** a Wolfram CDF file (<https://www.wolfram.com/cdf/>). This file runs freely under the CDF player and behaves exactly as the mathematica cell with the interactive code. The only difference is that I/O is not permitted and the catalog is hardcoded inside the CDF. This file is provided to allow user without a wolfram license to test the GUI functionality.

* the python porting of the core scanning algorithm, added in two formats: 
   * **python\_code.py**. This is a plain python code that needs local interpreter and uses some external packages that must be installed in the local machine. It can easily be done with standard python package managers like *pip* or conda <https://docs.conda.io/en/latest/>. File **conda\_env\_export.yml**     contains some info about the environment under which the software actually runs.
   * **multiplets_core_colab.ipynb** is same code but in the *ipython version*. It runs in Jupyter (<https://jupyter.org/>), a web based IDE very similar to the mathematica model. A great aspect of this web paradigm is that it is possible to run the program in a cloud environment like Google Colab (<https://colab.google>), without the need to have a python interpreter locally installed. The python behave exactly as the mathematica code, but has No GUI: I/O is furthermore based on disk files read-write. Any user interface can be easily added by experienced Python programmers. 

* **Simulated_100k_005-02-2_geo.txt.zip**: an *example catalog file* for code testing, in the format needed by the software importer. 
The file is a table of 6 columns, each row being a seismic event. The six values , in order from left to right, are:
1) Time in decimal years, 2) latitude in decimal degrees 3) longitude in decimal degrees, 4) depth in km (positive numbers, higher number is deeper), 5) magnitude, 6) free event ID, this last one is left for any user reference need.
The last ID item is not used by the software, but the importer expects a number to be present.
* **ascii_files.txt**: zip archive containing the files generated by the python software on the given example catalog when parameters are set to (see also reproducible snapshot yaml files, see below):
  * removed = "GK"
  * gkrad = "sum"; 
  * magthresh=5.5
  * dmplus=.4
  * dmminus=.6

They are present for reference only: in particular 
* **python_mp.txt** contains the mp vector extracted in the python porting when the magnitude prefiltering is applied to the catalog
* **python_orig.txt** contains the internally complete converted catalog table where coordinates are referred to a local cartesian coordinate system (the exact origin is not important since filtering is dependent upon *relative* coordinates). 
  The last column of the converted table is the row ID of the complete catalog and is used by the internal routines
* **python_vs_out_new.txt** is the complete result of the cluster search. It contains catalog file and searching parameters info, counts of found multiplets together with detailed lists of the event IDs clustered together. 

* **conda_env_export.yml**: the python code uses some packages installed in our case with conda (<https://docs.conda.io/en/latest/>) in a local environment, this file is the Conda export of the environment under which the software actually runs. Even if it is not straightforward to reconstruct the python environment from this file using Conda, it can anyway give some useful info.

As a last add-on, using the "reproducible" (<https://pypi.org/project/reproducible/>) package, two yaml snapshot files have been generated at runtime (and the code to do this has been left commented) and added to give informations on the actual working environment under which the file runs.

execution time test have been done on a modern Mac arm M1 processor: the rather big synthetic catalog example file contains 337254 events from which a subset of 95807 events is prefiltered using the parameters above. Search time on this subset gives out 2123 cluster,  found in about 2 minutes and half of processing time. The mathematica time of execution for the same dataset is around 20 seconds.

The code is open source, not maintained and hopefully "citation-ware" i.e. it can be used completely free, and a citation to the author is more than welcome.
